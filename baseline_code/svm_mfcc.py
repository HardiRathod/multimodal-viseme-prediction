# -*- coding: utf-8 -*-
"""SVM_MFCC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9bxhZC5UdQIlMuDHAyWvOL5S5QNBUIR
"""

import pandas as pd
import numpy as np

import sklearn
from sklearn.svm import SVC

data = pd.read_csv('/content/drive/Shareddrives/CS535 Project/mfcc_feature_set.csv', header = None)

data.head(10)

data = data.dropna()
data.head(10)

from scipy.stats import pearsonr
import matplotlib.pyplot as plt 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as sfa
import random
import numpy as np
from sklearn import preprocessing as ppg
from sklearn.metrics import mean_squared_error as mse
from sklearn.preprocessing import normalize as nm
from sklearn.neighbors import KNeighborsRegressor as knn
def correlation(x_var, y_var, ax=None, **kws):
    r, m = pearsonr(x_var, y_var)
    ax = ax or plt.gca()
    ax.annotate(f'c = {r:.2f}', xy=(.1, .8), xycoords=ax.transAxes)
g = sns.pairplot(data, diag_kind = 'kde')
g.map_lower(correlation)
plt.show()

data_intermed = data[data[20]!= '_']

k = data_intermed[20].value_counts()
type(k)

classes = k.index.values
classes

import matplotlib.pyplot as plt
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(k.index.values, k.tolist(), color ='blue',
        width = 0.6)
 
plt.xlabel("Viseme Classes")
plt.ylabel("No. of examples")
plt.title("# Examples for viseme classes")
plt.show()

viseme_class = []
average_mfcc_

data = data[data[20]!= 'Oy']

data = data[data[20]!= '_']

from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size = 0.3, stratify=data[[20]])

train, validate, test = \
              np.split(data.sample(frac=1, random_state=42), 
                       [int(.7*len(data)), int(.85*len(data))])

train

validate

test

train_y = train[[20]]
train_y

train_x = train.drop(columns = [20])
train_x

data[[i for i in range(20)]]

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC

# To apply an classifier on this data, we need to flatten the image, to
y_data = data[[20]]

# Split the dataset in two equal parts
X_train, X_test, y_train, y_test = train_test_split(data[[i for i in range(20)]], data[[20]], test_size=0.3, random_state=0, stratify = data[[20]])

# Set the parameters by cross-validation
tuned_parameters = [
    {"kernel": ["rbf"], "gamma": [1e-3, 1e-4], "C": [1, 10, 100, 1000]},
    {"kernel": ["linear"], "C": [1, 10, 100, 1000]},
]

scores = ["precision", "recall"]

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    clf = GridSearchCV(SVC(), tuned_parameters, scoring="%s_macro" % score)
    clf.fit(X_train, y_train)
    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_["mean_test_score"]
    stds = clf.cv_results_["std_test_score"]
    
    for mean, std, params in zip(means, stds, clf.cv_results_["params"]):
        print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
    print()
    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, clf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print()

import sklearn
from sklearn.svm import LinearSVC
svclassifier = LinearSVC()
svclassifier.fit(train_x, train_y.values)

test_y = test[[20]]
test_x = test.drop(columns = [20])
predictions = svclassifier.predict(test_x)

predictions

test_y

from sklearn.metrics import accuracy_score
accuracy_score(test_y, predictions)

from sklearn.metrics import f1_score
f1_score(test_y, predictions, average = 'micro')

from sklearn.metrics import recall_score
recall_score(test_y, predictions, average = 'micro')

